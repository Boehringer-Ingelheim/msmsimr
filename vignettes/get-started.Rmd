---
title: "get-started"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{get-started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 7/1.61,
  fig.align = "center"
)
```

```{r setup}
options(tidyverse.quite = TRUE)
library(tidyverse, warn.conflicts = TRUE)

library(future)
plan(multisession)

library(simmsm)
```

Oncology studies often employ surrogate endpoints such as the response rate
or progression free survival at interim analyses to drive

1. futility stopping
2. requesting accelerated approval.

In a confirmatory setting this often requires alpha allocation to the different
endpoints of interest to maintain strict control of the maximal type one error
rate.

The optimal multiple-testing strategy, the timing of interim analyses,
and futility stopping boundaries depend both on the study objectives and 
the complex relationship between response, progression, and death.

Simulating detailed patient-level data thus allows to investigate operating
characteristics in more detail and can drive smart risk taking.

One class of models proposed in this context are multi-state survival models (MSM).
Consider, for instance, the following MSM.

```{r, echo=FALSE}
nomnoml::nomnoml(
"[<state>start]
[start]-[<note>1]
[1]->[<state>response]
[start]-[<note>2]
[2]->[<state>progression]
[response]-[<note>3]
[3]->[progression]
[progression]-[<note>4]
[4]->[<state>death]", height = 200, width = 700
)
```

This model contains all states relevant to response rate, PFS, OS
(and duration of response). 
Assuming a low rate of direct transitions from start/response to death,
the model does not include direct transitions from start/response to death
to reduce complexity. 

Both the transition matrix and the per-transition survival time distributions
need to be defined.
Here, the parameters are chosen heuristically.

```{r}
tmat <- as_TransitionMatrix(
  matrix(c(
    NA,  1,  2, NA,
    NA, NA,  3, NA,
    NA, NA, NA,  4,
    NA, NA, NA, NA
  ), nrow = 4, byrow = TRUE), 
  state_labels = c("start", "response", "progression", "death")
)
tmat

msm <- MSM(
  Exponential(log(2)/3),
  Exponential(log(2)/6),
  Exponential(log(2)/12),
  Exponential(log(2)/3),
  tmat = tmat,
  tmax = 60
)
```

It is then possible to simulate trajectories from the model
```{r}
tbl_data <- simulate(msm, nsim = 200)
tbl_data
```

These can be used to estimate time-to-first event for single or compound
endpoints.

```{r}
plot_model <- function(data) {
  old_pars <- par(mfrow = c(2, 2), mar = c(2, 2, 2, 1))
  plot(kaplan_meier(data, "start", "response", FALSE), xlim = c(0, 36))
  title("response")
  plot(kaplan_meier(data, "progression", "death", FALSE), xlim = c(0, 36))
  title("progression->death")
  plot(kaplan_meier(data, "start", c("progression", "death"), FALSE), xlim = c(0, 36))
  title("PFS")
  plot(kaplan_meier(data, "start", "death", FALSE), xlim = c(0, 36))
  title("OS")
  par(old_pars)
}
plot_model(tbl_data)
```

The same data can be used to estimate next state probabilities.

```{r}
next_state_probabilities(msm, tbl_data, as_matrix = TRUE)
```

Assumptions about treatment effects can be incorporated by modifying the
cumulative transition hazards of the model.

```{r}
hrmat <- tmat
hrmat["start", "response"] <- 1
hrmat["start", "progression"] <- .6
hrmat["response", "progression"] <- .5
hrmat["progression", "death"] <- .6
hrmat

idx <- which(!is.na(as.vector(tmat)))
idx <- idx[order(tmat[idx])]
hrvec <- hrmat[idx]
```

```{r}
tbl_data <- simulate(msm, nsim = 200, hazard_ratios = hrvec)

plot_model(tbl_data)
```

In practice, it is often difficult to
access suitable individual-level data to estimate the parameters for a planned
control group directly.
An alternative are response probabilities and Kaplan-Meier plots
of PFS and OS published in the literature.

Consider the following data that was generated from the model above and could
have been extracted from a research article.

```{r}
data("example_calibration_data")
example_calibration_data
```

The data can be parsed in 'calibration objects' to calculate an approximate
likelihood under a set of model parameters.
The likelihood is constructed assuming that the standardized difference in
between the reported and the model-fitted survival distribution are
(independently) t-distributed.

```{r}
cal1 <- with(example_calibration_data,
    calibration_probability(
      "start", "response", pr_start_response, pr_start_response_se
    )
  )
cal2 <- with(example_calibration_data$tbl_km_pfs,
    calibration_survival_curve(
      "start", c("progression", "death"), time = t, survival = est, standard_error = se
    )
  )
cal3 <- with(example_calibration_data$tbl_km_os,
    calibration_survival_curve(
      "start", "death", time = t, survival = est, standard_error = se
    )
  )

lls <- purrr::map_dbl(1:5, ~log_likelihood(msm, cal1, cal2, cal3))
mean(lls)
sd(lls)/sqrt(5) # not ideal - quite variable; bump nsim? needs to be a lot faster
```

This approximate likelihood can be optimized.

First, need to make parameters tunable. PFS/OS and response rate alone are
insufficient to identify all transitions.
Need to keep one of the 'response' transitions and the post-progression
transitions fixed.

```{r}
as_vector(msm)

msm2 <- MSM(
  Exponential(log(2)/12, optimize = TRUE),
  Exponential(log(2)/12, optimize = TRUE),
  Exponential(log(2)/12),
  Exponential(log(2)/3),
  tmat = tmat,
  tmax = 60
)

as_vector(msm2)
as_vector(update_parameters(msm2, c(1, 1)))
```

```{r, eval=FALSE}
negative_log_likelihood <- function(x) { # minimization - multiply with -1
  res <- -log_likelihood(
      update_parameters(msm2, x),
      cal1, cal2, cal3, seed = 42L
    )
  return(res)
}

# the gradient is super expensive with more parameters
gradient <- function(x, eps = 0.025) { # use relative large eps to make robust vs noise
  numDeriv::grad(negative_log_likelihood,
    x, method = "simple", method.args = list(eps = eps))
}

true_values<- unlist(msm)[c(1, 2)]
true_values
# some start value
initial_values <- as_vector(msm2)
initial_values
# check objective
negative_log_likelihood(initial_values)
negative_log_likelihood(true_values)
# check initial gradient
gradient(initial_values)
```

Let's have a look at the start.

```{r, eval=FALSE}
plot_pfs_os <- function(data1, data2) {
  tbl <- bind_rows(
    kaplan_meier(data1, "start", c("progression", "death")) %>% 
      mutate(model = "A", endpoint = "PFS"),
    kaplan_meier(data2, "start", c("progression", "death")) %>% 
      mutate(model = "B", endpoint = "PFS"),
    kaplan_meier(data1, "start", "death") %>% 
      mutate(model = "A", endpoint = "OS"),
    kaplan_meier(data2, "start", "death") %>% 
      mutate(model = "B", endpoint = "OS")
  )
  ggplot(tbl) +
    aes(time) +
    geom_step(aes(y = estimate, color = model)) +
    facet_wrap(~endpoint) 
}

data_fix <- simulate(msm)
new_data <- simulate(msm2)
plot_pfs_os(data_fix, new_data)

next_state_probabilities(msm, data_fix, as_matrix = TRUE)
next_state_probabilities(msm, new_data, as_matrix = TRUE)
```


Try good old gradient descent first.

```{r, eval=FALSE}
res <- simmsm:::gradient_descent(
  negative_log_likelihood,
  gradient,
  init = initial_values,
  lower = rep(log(2)/36, 2),
  upper = rep(log(2)/1, 2),
  rate = 1/1000,
  momentum = 0.25,
  niter = 20
)

opt <- res[[1]][which.min(res[[2]]), ]
round(opt, 3)
```

```{r, eval=FALSE}
new_data <- simulate(update_parameters(msm2, opt))
plot_pfs_os(data_fix, new_data)

next_state_probabilities(msm, data_fix, as_matrix = TRUE)
next_state_probabilities(msm, new_data, as_matrix = TRUE)
```

Working ok, try different optimizers etc.

The fitted model can then be used to simulate multi-state IDM data.

ToDo:

1. more distributions (generalized gamma, generalized F -> flexsurv)
2. allow 'custom' distribution (no parameters) via cumulative hazard directly;
  allows to estimate some or all of the transition hazards from ILD and plug in
3. Speed up mstate::mssample in C++
4. Can we get an approximate Hessian? Yes, too expensive / unprecise?
5. Explain how an extended MSM can be used to simulate under transition specific censoring.
