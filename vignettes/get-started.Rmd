---
title: "get-started"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{get-started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  # eval = FALSE, # nolint
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 7 / 1.61,
  fig.align = "center"
)
```

```{r setup}
options(tidyverse.quiet = TRUE)
library(tidyverse, warn.conflicts = TRUE)

library(future)
plan(multisession)

library(msmsimr)
```

Work In Progress!

```{r, echo=FALSE}
nomnoml::nomnoml(
"[<state>start]
[start]-[<note>1]
[1]->[<state>response]
[start]-[<note>2]
[2]->[<state>progression]
[response]-[<note>3]
[3]->[progression]
[progression]-[<note>4]
[4]->[<state>death]", height = 200, width = 700
)
```

This model contains all states relevant to response rate, PFS, OS
(and duration of response). 
Assuming a low rate of direct transitions from start/response to death,
the model does not include direct transitions from start/response to death
to reduce complexity. 

Both the transition matrix and the per-transition survival time distributions
need to be defined.
Here, the parameters are chosen heuristically.

```{r}
tmat <- as_TransitionMatrix(
  matrix(c(
    NA,  1,  2, NA,
    NA, NA,  3, NA,
    NA, NA, NA,  4,
    NA, NA, NA, NA
  ), nrow = 4, byrow = TRUE),
  state_labels = c("start", "response", "progression", "death")
)
tmat

msm <- MSM(
  Exponential(log(2) / 3),
  Exponential(log(2) / 6),
  Exponential(log(2) / 12),
  Exponential(log(2) / 3),
  tmat = tmat,
  tmax = 60
)
```

It is then possible to simulate trajectories from the model

```{r}
tbl_data <- simulate(msm, nsim = 200)
tbl_data
```

These can be used to estimate time-to-first event for single or compound
endpoints.

```{r}
plot_model <- function(data) {
  old_pars <- par(mfrow = c(2, 2), mar = c(2, 2, 2, 1))
  plot(kaplan_meier(msm, "start", "response", data, FALSE), xlim = c(0, 36))
  title("response")
  plot(kaplan_meier(msm, "progression", "death", data, FALSE), xlim = c(0, 36))
  title("progression->death")
  plot(kaplan_meier(msm, "start", c("progression", "death"), data, FALSE),
    xlim = c(0, 36))
  title("PFS")
  plot(kaplan_meier(msm, "start", "death", data, FALSE), xlim = c(0, 36))
  title("OS")
  par(old_pars)
}
plot_model(tbl_data)
```

The same data can be used to estimate next state probabilities.

```{r}
next_state_probabilities(msm, tbl_data, as_matrix = TRUE)
```

Assumptions about treatment effects can be incorporated by modifying the
cumulative transition hazards of the model.

```{r}
hrmat <- tmat
hrmat["start", "response"] <- 1
hrmat["start", "progression"] <- .6
hrmat["response", "progression"] <- .5
hrmat["progression", "death"] <- .6
hrmat

idx <- which(!is.na(as.vector(tmat)))
idx <- idx[order(tmat[idx])]
hrvec <- hrmat[idx]
```

```{r}
tbl_data <- simulate(msm, nsim = 200, hazard_ratios = hrvec)

plot_model(tbl_data)
```

An alternative to individual-level data for fitting a MSM are response
probabilities and Kaplan-Meier plots of PFS and OS published in the literature.

Consider the following data that was generated from the model above and could
have been extracted from a research article.

```{r}
data("example_calibration_data")
example_calibration_data
```

TODO: unnecessarily complex, remove

The data can be parsed in 'calibration objects' to calculate an approximate
likelihood under a set of model parameters.
The likelihood is constructed assuming that the standardized difference in
between the reported and the model-fitted survival distribution are
(independently) t-distributed.

```{r}
cal1 <- with(example_calibration_data,
    calibration_probability(
      "start", "response", pr_start_response, pr_start_response_se
    )
  )
cal2 <- with(example_calibration_data$tbl_km_pfs,
    calibration_survival_curve(
      "start", c("progression", "death"), time = t, survival = est,
      standard_error = se
    )
  )
cal3 <- with(example_calibration_data$tbl_km_os,
    calibration_survival_curve(
      "start", "death", time = t, survival = est, standard_error = se
    )
  )
```

```{r}
lls <- purrr::map_dbl(1:5, ~log_likelihood(msm, cal1, cal2, cal3))
mean(lls)
sd(lls) / sqrt(5) # not ideal - quite variable; bump nsim? needs to be faster
```

This approximate likelihood can be optimized.

First, need to make parameters tunable. PFS/OS and response rate alone are
insufficient to identify all transitions, so assuming we have additional evidence
on progression -> death and response -> progression (assumed fixed).
Need to keep one of the 'response' transitions and the post-progression
transitions fixed to make this identifiable.

```{r}
to_vector(msm)

msm2 <- MSM(
  Exponential(log(2) / 12, optimize = TRUE),
  Exponential(log(2) / 12, optimize = TRUE),
  Exponential(log(2) / 12),
  Exponential(log(2) / 3),
  tmat = tmat,
  tmax = 60
)

to_vector(msm2)
to_vector(update_parameters(msm2, c(1, 1)))
```

We minimize the negative log likelihood.
The problem is the gradient which can only be approximated via finite
differences and sampling.
This is slow and imprecise.

```{r}
negative_log_likelihood <- function(x) { # minimization - multiply with -1
  res <- -log_likelihood(
      update_parameters(msm2, x),
      cal1, cal2, cal3, seed = 42L
    )
  return(res)
}

# the gradient is super expensive with more parameters
gradient <- function(x, eps = 0.1) { # use relative large eps to make robust
  numDeriv::grad(negative_log_likelihood,
    x, method = "simple", method.args = list(eps = eps))
}

true_values <- unlist(msm)[c(1, 2)]
true_values
# some start value
initial_values <- to_vector(msm2)
initial_values
# check objective
negative_log_likelihood(initial_values)
negative_log_likelihood(true_values)
# check initial gradient
gradient(initial_values)
```

Plot starting values.

```{r}
plot_pfs_os <- function(msm1, data1, msm2, data2) {
  tbl <- bind_rows(
    kaplan_meier(msm1, "start", c("progression", "death"), data = data1) %>%
      mutate(model = "A", endpoint = "PFS"),
    kaplan_meier(msm2, "start", c("progression", "death"), data = data2) %>%
      mutate(model = "B", endpoint = "PFS"),
    kaplan_meier(msm1, "start", "death", data = data1) %>%
      mutate(model = "A", endpoint = "OS"),
    kaplan_meier(msm2, "start", "death", data = data2) %>%
      mutate(model = "B", endpoint = "OS")
  )
  ggplot(tbl) +
    aes(time) +
    geom_step(aes(y = estimate, color = model)) +
    facet_wrap(~endpoint)
}

data_fix <- simulate(msm)
new_data <- simulate(msm2)
plot_pfs_os(msm, data_fix, msm2, new_data)

next_state_probabilities(msm, data_fix, as_matrix = TRUE)
next_state_probabilities(msm2, new_data, as_matrix = TRUE)
```

Try good old gradient descent first.

```{r}
set.seed(42L)

res <- simmsm:::gradient_descent(
  negative_log_likelihood,
  gradient,
  init = initial_values,
  rate = 1 / 10,
  niter = 20
)

opt <- res[[1]][which.min(res[[2]]), ]
round(opt, 3)
```

Compare solution with ground truth.

```{r}
new_data <- simulate(update_parameters(msm2, opt))
plot_pfs_os(msm, data_fix, msm2, new_data)

next_state_probabilities(msm, data_fix, as_matrix = TRUE)
next_state_probabilities(msm2, new_data, as_matrix = TRUE)
```

Working ok, try different optimizers?

Advantage of this setup: can plug-in distributions fitted with different 
packages (just provide cum haz table); only need to fill in the blanks.
Complete;y arbitrary MSM structure.

The fitted model can then be used to simulate multi-state IDM data.

ToDo:

1. more distributions (generalized gamma, generalized F -> flexsurv)
2. allow 'custom' distribution (no parameters) via cumulative hazard directly;
  allows to estimate some or all of the transition hazards from ILD and plug in
3. Speed up mstate::mssample in C++
4. Can we get an approximate Hessian? Yes, too expensive / unprecise?
5. Explain how an extended MSM can be used to simulate under transition specific censoring.
6. Use reparametrization trick for efficient maximum likelihood? 
https://gabrielhuang.gitbooks.io/machine-learning/content/reparametrization-trick.html
